{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpNizYwGObA0pU87eauZsJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s183796/Group5_repos/blob/main/3D_unet_full_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5K-M-WHe_zR",
        "outputId": "bc6c5ae6-4a37-4e60-dba0-e96443a3ef41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "loaded packages\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display, clear_output\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn.functional import softmax\n",
        "import PIL.Image\n",
        "import os\n",
        "import torchvision\n",
        "import cv2\n",
        "\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "print('loaded packages')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MkvoH-Ff3VO",
        "outputId": "3d90e3e9-d5f9-4d2a-9dcb-99287631d677"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchio\n",
            "  Downloading torchio-0.19.3-py2.py3-none-any.whl (172 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/173.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m163.8/173.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.0/173.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from torchio)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting SimpleITK!=2.0.*,!=2.1.1.1 (from torchio)\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from torchio) (4.7.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from torchio) (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from torchio) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torchio) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.10/dist-packages (from torchio) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchio) (4.66.1)\n",
            "Requirement already satisfied: typer[all] in /usr/local/lib/python3.10/dist-packages (from torchio) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (2.1.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->torchio) (1.14.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel->torchio) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->torchio) (67.7.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]->torchio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]->torchio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]->torchio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]->torchio) (13.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]->torchio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]->torchio) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1->torchio) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1->torchio) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]->torchio) (0.1.2)\n",
            "Installing collected packages: SimpleITK, shellingham, Deprecated, colorama, torchio\n",
            "Successfully installed Deprecated-1.2.14 SimpleITK-2.3.1 colorama-0.4.6 shellingham-1.5.4 torchio-0.19.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3, visited the 16th of November 2023\n",
        "# Modifications have been made to the original code with changing the input and output sizes\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        # input: 1x128x128\n",
        "        self.e11 = nn.Conv3d(1, 64, kernel_size=3,padding=1)\n",
        "        self.e12 = nn.Conv3d(64, 64, kernel_size=3,padding=1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2) #64x64x64\n",
        "\n",
        "        self.e21 = nn.Conv3d(64, 128, kernel_size=3,padding=1)\n",
        "        self.e22 = nn.Conv3d(128, 128, kernel_size=3,padding=1)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2) #32x32x128\n",
        "\n",
        "        self.e31 = nn.Conv3d(128, 256, kernel_size=3,padding=1)\n",
        "        self.e32 = nn.Conv3d(256, 256, kernel_size=3,padding=1)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2) #16x16x256\n",
        "\n",
        "        self.e41 = nn.Conv3d(256, 512, kernel_size=3,padding=1)\n",
        "        self.e42 = nn.Conv3d(512, 512, kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose3d(512,256,kernel_size=2,stride=2) #\n",
        "        self.d21 = nn.Conv3d(512,256,kernel_size=3,padding=1)\n",
        "        self.d22 = nn.Conv3d(256,256,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose3d(256,128,kernel_size=2,stride=2)\n",
        "        self.d31 = nn.Conv3d(256,128,kernel_size=3,padding=1)\n",
        "        self.d32 = nn.Conv3d(128,128,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose3d(128,64,kernel_size=2,stride=2)\n",
        "        self.d41 = nn.Conv3d(128,64,kernel_size=3,padding=1)\n",
        "        self.d42 = nn.Conv3d(64,64,kernel_size=3,padding=1)\n",
        "\n",
        "        self.outconv = nn.Conv3d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        xe11 = F.relu(self.e11(x))\n",
        "        xe12 = F.relu(self.e12(xe11))\n",
        "        xp1 = self.pool1(xe12)\n",
        "\n",
        "        xe21 = F.relu(self.e21(xp1))\n",
        "        xe22 = F.relu(self.e22(xe21))\n",
        "        xp2 = self.pool2(xe22)\n",
        "\n",
        "\n",
        "        xe31 = F.relu(self.e31(xp2))\n",
        "        xe32 = F.relu(self.e32(xe31))\n",
        "\n",
        "        xp3 = self.pool3(xe32)\n",
        "\n",
        "        xe41 = F.relu(self.e41(xp3))\n",
        "        xe42 = F.relu(self.e42(xe41))\n",
        "\n",
        "        # Decoder\n",
        "        xup2 = self.upconv2(xe42)\n",
        "        xcat2 = torch.cat([xup2, xe32], dim=1)\n",
        "\n",
        "\n",
        "        xup31 = F.relu(self.d21(xcat2))\n",
        "        xup32 = F.relu(self.d22(xup31))\n",
        "        xup3 = self.upconv3(xup32)\n",
        "        xcat3 = torch.cat([xup3, xe22], dim=1)\n",
        "\n",
        "        xup41 = F.relu(self.d31(xcat3))\n",
        "        xup42 = F.relu(self.d32(xup41))\n",
        "\n",
        "        xup4 = self.upconv4(xup42)\n",
        "        xcat4 = torch.cat([xup4, xe12], dim=1)\n",
        "        xup51 = F.relu(self.d41(xcat4))\n",
        "        xup52 = F.relu(self.d42(xup51))\n",
        "\n",
        "        out = self.outconv(xup52)\n",
        "\n",
        "        output = out\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"Defined Unet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUE3unVCfDCR",
        "outputId": "ce7fc6e1-5052-40aa-fb66-ed775ea2b03c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined Unet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up hyper parameters, from exercise week 6\n",
        "loss_fn =  nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "7xzJxGxVfMf3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dataset\n",
        "class SOCDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_folder = os.path.join(root_dir, 'data/')\n",
        "        self.label_folder = os.path.join(root_dir, 'labels/')\n",
        "        self.image_filenames = sorted([f for f in os.listdir(self.image_folder) if f.endswith('.tiff')])\n",
        "        self.label_filenames = sorted([f for f in os.listdir(self.label_folder) if f.endswith('.tif')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_name = os.path.join(self.image_folder, self.image_filenames[idx])\n",
        "\n",
        "      number1=img_name[-8:-5] #Making sure image and label match\n",
        "      label_name=os.path.join(self.label_folder,'slice__'+str(number1)+'.tif')\n",
        "\n",
        "      image = cv2.imread(img_name, cv2.IMREAD_GRAYSCALE)\n",
        "      label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n",
        "      image=torch.from_numpy(np.array(image))\n",
        "      label=torch.from_numpy(np.array(label))\n",
        "\n",
        "      return image, label\n"
      ],
      "metadata": {
        "id": "ajvzgy6tfPiu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOC_dataset = SOCDataset(root_dir='drive/My Drive//AI data/') #change path if running on the HPC\n",
        "\n",
        "print('loaded data')\n",
        "\n",
        "images=[]\n",
        "labels=[]\n",
        "for i in range(len(SOC_dataset)):\n",
        "  image,label=SOC_dataset[i]\n",
        "\n",
        "  images.append(image)\n",
        "  labels.append(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B64bQe9Ffpau",
        "outputId": "83af2245-74cc-4c74-cf97-b0358b7d30cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_cube(volume,cube_size):\n",
        "\n",
        "  vol_split=volume.unfold(2, cube_size, cube_size).unfold(1, cube_size, cube_size).unfold(0, cube_size, cube_size)\n",
        "\n",
        "  vol_split=vol_split.reshape([vol_split.size(0)**3,cube_size,cube_size,cube_size])\n",
        "  return vol_split"
      ],
      "metadata": {
        "id": "EVi-CK8HfXRB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating large volumes\n",
        "im_vol=torch.stack(images)\n",
        "labels_vol=torch.stack(labels)\n",
        "\n",
        "#Creating sub volumes\n",
        "im_vol=split_cube(im_vol,64)\n",
        "label_vol=split_cube(labels_vol,64)"
      ],
      "metadata": {
        "id": "FirCyVsNfxrk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchio as tio\n",
        "from re import I\n",
        "\n",
        "#Splitting data in images and labelled images\n",
        "#This is implemented with the torchio workflow\n",
        "elements = []\n",
        "for i in range(im_vol.size(0)):\n",
        "    element = tio.Subject(\n",
        "        image_sub=tio.ScalarImage(tensor=im_vol.unsqueeze(0)[:,i,:,:]),\n",
        "        label_sub=tio.LabelMap(tensor=label_vol.unsqueeze(0)[:,i,:,:]),\n",
        "    )\n",
        "    elements.append(element)\n",
        "dataset = tio.SubjectsDataset(elements)"
      ],
      "metadata": {
        "id": "zvH5rQKmfxo0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Adding normalization of pixel intensities,\n",
        "transforms = (\n",
        "    tio.ZNormalization(masking_method=tio.ZNormalization.mean)\n",
        ")\n",
        "\n",
        "transform = transforms"
      ],
      "metadata": {
        "id": "uK5qfme5fxil"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "339vDMpEfxRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting in training, validation and test data\n",
        "\n",
        "#splitting ratios\n",
        "training_size=0.7\n",
        "val_split=0.5\n",
        "\n",
        "#Find number of images in train, val and test\n",
        "train_number = int(training_size * len(dataset))\n",
        "val_number = len(dataset) - train_number\n",
        "test_number = val_number*val_split\n",
        "\n",
        "#splitting of training and validation set\n",
        "train_val = train_number, val_number\n",
        "train_images, val_images = torch.utils.data.random_split(elements, train_val)\n",
        "\n",
        "val_test = int(val_number*val_split), int(val_number*val_split)+1\n",
        "test_images,val_images = torch.utils.data.random_split(val_images,val_test)\n",
        "\n",
        "#Creating datasets with pixel normalization\n",
        "train_set = tio.SubjectsDataset(\n",
        "    train_images, transform=transform)\n",
        "\n",
        "val_set = tio.SubjectsDataset(\n",
        "    val_images, transform=transform)\n",
        "\n",
        "test_set = tio.SubjectsDataset(\n",
        "    test_images, transform=transform)\n",
      ],
      "metadata": {
        "id": "e_z-izFygGvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "num_workers = 4 #adapted to the HPC\n",
        "\n",
        "#Train batch size of 16, validation batch size of 32\n",
        "training_batch_size = 16\n",
        "validation_batch_size = 2 * training_batch_size\n",
        "\n",
        "#Splitting in training, test and validation\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=training_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=validation_batch_size,\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=validation_batch_size,\n",
        "    num_workers=num_workers,\n",
        ")"
      ],
      "metadata": {
        "id": "DMbiXjdAgGsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics\n"
      ],
      "metadata": {
        "id": "3qQ7Y0ZvgGo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parts of this code section is from exercise 4.2-EXE-CNN-CIFAR-10.ipynb\n",
        "net=UNet(n_class=3)\n",
        "\n",
        "from torchmetrics.classification import JaccardIndex\n",
        "from torchmetrics.functional.classification import dice\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "jaccard=JaccardIndex(task=\"multiclass\", num_classes=3).to(device)\n",
        "accuracy=MulticlassAccuracy(num_classes=3).to(device)\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "validation_every_steps = np.ceil(len(training_loader.dataset)/batch_size)\n",
        "\n",
        "step = 0\n",
        "net.train()\n",
        "\n",
        "train_accuracies_jaccard = []\n",
        "train_accuracies_dice = []\n",
        "train_accuracies_pixel = []\n",
        "valid_accuracies_jaccard = []\n",
        "valid_accuracies_dice = []\n",
        "valid_accuracies_pixel = []\n",
        "loss_train = []\n",
        "val_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_accuracies_batches_jaccard = []\n",
        "    train_accuracies_batches_dice = []\n",
        "    train_accuracies_batches_pixel = []\n",
        "    loss_epochs=[]\n",
        "\n",
        "    for subjects_batch in training_loader:\n",
        "        inputs = subjects_batch['image_sub'][tio.DATA].to(device)\n",
        "        targets = subjects_batch['label_sub'][tio.DATA].to(device)\n",
        "\n",
        "        #Forward pass\n",
        "        output = net(inputs)\n",
        "\n",
        "        un_target=targets.unique()\n",
        "        targets[targets==un_target[0]]=0\n",
        "        targets[targets==un_target[1]]=1\n",
        "        targets[targets==un_target[2]]=2\n",
        "\n",
        "        targets = targets.to(torch.int64) [:,0,:,:,:]\n",
        "\n",
        "        #Loss function\n",
        "        loss = loss_fn(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "        # Compute accuracy. Note the adding of the softmax function to obtain the probabilities\n",
        "        predictions = torch.argmax(softmax(output,dim=1),dim=1)\n",
        "        train_accuracies_batches_dice.append(dice(predictions,targets).cpu())\n",
        "        train_accuracies_batches_jaccard.append(jaccard(predictions,targets).cpu())\n",
        "        train_accuracies_batches_pixel.append(accuracy(predictions,targets).cpu())\n",
        "        loss_epochs.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        if step % validation_every_steps == 0:\n",
        "\n",
        "            # Append average training accuracy to list.\n",
        "            train_accuracies_jaccard.append(np.mean(train_accuracies_batches_jaccard))\n",
        "            train_accuracies_dice.append(np.mean(train_accuracies_batches_dice))\n",
        "            train_accuracies_pixel.append(np.mean(train_accuracies_batches_pixel))\n",
        "            loss_train.append(np.mean(loss_epochs))\n",
        "\n",
        "            train_accuracies_batches_jaccard = []\n",
        "            train_accuracies_batches_dice = []\n",
        "            train_accuracies_batches_pixel = []\n",
        "\n",
        "            # Compute accuracies on validation set.\n",
        "            valid_accuracies_batches_jaccard = []\n",
        "            valid_accuracies_batches_dice = []\n",
        "            valid_accuracies_batches_pixel = []\n",
        "            val_losses=[]\n",
        "            with torch.no_grad():\n",
        "                net.eval()\n",
        "                for subjects_batch in val_loader:\n",
        "                    inputs = subjects_batch['image_sub'][tio.DATA].to(device)\n",
        "                    targets = subjects_batch['label_sub'][tio.DATA].to(device)\n",
        "                    output = net(inputs)\n",
        "\n",
        "                    un_target=targets.unique()\n",
        "                    targets[targets==un_target[0]]=0\n",
        "                    targets[targets==un_target[1]]=1\n",
        "                    targets[targets==un_target[2]]=2\n",
        "\n",
        "                    targets = targets.to(torch.int64) [:,0,:,:,:]\n",
        "\n",
        "                    loss = loss_fn(output, targets)\n",
        "\n",
        "                    predictions = torch.argmax(softmax(output,dim=1),dim=1)\n",
        "                    valid_accuracies_batches_dice.append(dice(predictions,targets).cpu())\n",
        "                    valid_accuracies_batches_jaccard.append(jaccard(predictions,targets).cpu())\n",
        "                    valid_accuracies_batches_pixel.append(accuracy(predictions,targets).cpu())\n",
        "                    val_losses.append(loss.cpu())\n",
        "\n",
        "                net.train()\n",
        "\n",
        "            # Append average validation accuracy to list.\n",
        "            valid_accuracies_jaccard.append(np.sum(valid_accuracies_batches_jaccard) / len(test_loader))\n",
        "            valid_accuracies_dice.append(np.sum(valid_accuracies_batches_dice) / len(test_loader))\n",
        "            valid_accuracies_pixel.append(np.sum(valid_accuracies_batches_pixel) / len(test_loader))\n",
        "            val_loss.append(np.mean(val_losses))\n",
        "\n",
        "            print(f\"Step {step:<5}   training accuracy with jaccard: {train_accuracies_jaccard[-1]}\")\n",
        "            print(f\"             training accuracy with dice: {train_accuracies_dice[-1]}\")\n",
        "            print(f\"             training accuracy with pixel by pixel: {train_accuracies_pixel[-1]}\")\n",
        "            print(f\"             validation accuracy with jaccard: {valid_accuracies_jaccard[-1]}\")\n",
        "            print(f\"             validation accuracy with dice: {valid_accuracies_dice[-1]}\")\n",
        "            print(f\"             validation accuracy with pixel by pixel: {valid_accuracies_pixel[-1]}\")\n",
        "            print(f\"             loss in validation: {val_loss[-1]}\")\n",
        "\n",
        "\n",
        "print(\"Finished training.\")\n",
        "\n",
        "print(f\"train_accuracies_jaccard: {train_accuracies_jaccard}\")\n",
        "print(f\"train_accuracies_dice: {train_accuracies_dice}\")\n",
        "print(f\"train_accuracies_pixel: {train_accuracies_pixel}\")\n",
        "print(f\"valid_accuracies_jaccard: {valid_accuracies_jaccard}\")\n",
        "print(f\"valid_accuracies_dice: {valid_accuracies_dice}\")\n",
        "print(f\"valid_accuracies_pixel: {valid_accuracies_pixel}\")"
      ],
      "metadata": {
        "id": "Lpkl4z8cgGhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pylab as pylab\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "fig,axs=plt.subplots(1,2,figsize=(15,5), dpi = 200)\n",
        "\n",
        "axs[0].plot(train_accuracies_dice,color='red',label='Dice')\n",
        "axs[0].plot(train_accuracies_jaccard,color='blue',label='Jaccard')\n",
        "axs[0].plot(train_accuracies_pixel,color='green',label='Pixel-wise')\n",
        "axs[0].grid()\n",
        "axs[0].legend()\n",
        "axs[0].set_title('Training accuracies')\n",
        "axs[0].set_xlabel('Epoch number')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_ylim([0,1])\n",
        "\n",
        "axs[1].plot(valid_accuracies_dice,color='red',label='Dice')\n",
        "axs[1].plot(valid_accuracies_jaccard,color='blue',label='Jaccard')\n",
        "axs[1].plot(valid_accuracies_pixel,color='green',label='Pixel-wise')\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "axs[1].set_title('Validation accuracies')\n",
        "axs[1].set_xlabel('Epoch number')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_ylim([0,1])\n",
        "plt.savefig('Train_Valid_acc.png')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "7RnT6xCwgTDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "fig,axs=plt.subplots(1,3,figsize=(15,5), dpi = 200)\n",
        "axs[0].plot(train_accuracies_dice,color='red',label='Training accuracy')\n",
        "axs[0].plot(valid_accuracies_dice,color='blue',label='Validation accuracy')\n",
        "axs[0].set_title('Dice coefficient accuracies')\n",
        "axs[0].set_xlabel('Epoch number')\n",
        "axs[0].grid()\n",
        "axs[0].legend()\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_ylim([0,1])\n",
        "\n",
        "axs[1].plot(train_accuracies_jaccard,color='red',label='Training accuracy')\n",
        "axs[1].plot(valid_accuracies_jaccard,color='blue',label='Validation accuracy')\n",
        "axs[1].set_title('Jaccard coefficient accuracies')\n",
        "axs[1].set_xlabel('Epoch number')\n",
        "axs[1].grid()\n",
        "axs[1].legend()\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_ylim([0,1])\n",
        "\n",
        "axs[2].plot(train_accuracies_pixel,color='red',label='Training accuracy')\n",
        "axs[2].plot(valid_accuracies_pixel,color='blue',label='Validation accuracy')\n",
        "axs[2].set_title('Pixel-wise coefficient accuracies')\n",
        "axs[2].set_xlabel('Epoch number')\n",
        "axs[2].grid()\n",
        "axs[2].legend()\n",
        "axs[2].set_ylabel('Accuracy')\n",
        "axs[2].set_ylim([0,1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('Accuracy_metrics.png')"
      ],
      "metadata": {
        "id": "qFYmn-0IgTAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "fig,axs=plt.subplots(1,1,figsize=(5,5), dpi = 200)\n",
        "axs.plot(loss_train,color='blue',label='Training loss')\n",
        "axs.plot(val_loss,color='red',label='Validation loss')\n",
        "axs.legend()\n",
        "axs.grid()\n",
        "axs.set_title('Loss')\n",
        "axs.set_xlabel('Epoch number')\n",
        "axs.set_ylabel('Loss')\n",
        "#axs.set_ylim([0,1])\n",
        "plt.tight_layout()\n",
        "plt.savefig('Loss.png')\n",
        "\n",
        "print(\"Producing 3D figures...\")\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "test_accuracies_dice=0\n",
        "test_accuracies_jaccard=0\n",
        "test_accuracies_pixel=0\n",
        "l_test=0\n",
        "\n",
        "\n",
        "for subjects_batch in test_loader:\n",
        "  inputs = subjects_batch['image_sub'][tio.DATA].to(device)\n",
        "  targets = subjects_batch['label_sub'][tio.DATA].to(device)\n",
        "  output = net(inputs)\n",
        "\n",
        "  un_target=targets.unique()\n",
        "  targets[targets==un_target[0]]=0\n",
        "  targets[targets==un_target[1]]=1\n",
        "  targets[targets==un_target[2]]=2\n",
        "\n",
        "  targets = targets.to(torch.int64) [:,0,:,:,:]\n",
        "\n",
        "  #Computing test accuracies\n",
        "  predicted = torch.argmax(softmax(output,dim=1),dim=1)\n",
        "  predicted=predicted.to(torch.int64)\n",
        "  for i in range(predicted.size(0)):\n",
        "    test_accuracies_dice+=dice(predicted[i,:,:,:],targets[i,:,:,:])\n",
        "    test_accuracies_jaccard+=jaccard(predicted[i,:,:,:],targets[i,:,:,:])\n",
        "    test_accuracies_pixel+=accuracy(predicted[i,:,:,:],targets[i,:,:,:])\n",
        "  l_test+=predicted.size(0) #save size of batchsize\n",
        "\n",
        "#Average test accuracy\n",
        "test_dice=(test_accuracies_dice/l_test).cpu()\n",
        "test_jaccard=(test_accuracies_jaccard/l_test).cpu()\n",
        "test_pixel=(test_accuracies_pixel/l_test).cpu()\n",
        "\n",
        "print(\"Finished test loader\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iyqfxhKIgS8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"test_accuracies_dice: {test_dice}\")\n",
        "print(f\"test_accuracies_jaccard: {test_jaccard}\")\n",
        "print(f\"test_accuracies_pixel: {test_pixel}\")"
      ],
      "metadata": {
        "id": "fHdQQLdKsttr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label1=output[0,0,:,:,:].cpu().detach().numpy()\n",
        "label2=output[0,1,:,:,:].cpu().detach().numpy()\n",
        "label3=output[0,2,:,:,:].cpu().detach().numpy()\n",
        "\n",
        "predicted=predicted.cpu().detach().numpy() [0,:,:,:]\n",
        "\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (5, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "x, y, z = np.meshgrid(np.arange(label1.shape[0]),np.arange(label1.shape[1]),np.arange(label1.shape[2]), indexing='ij')\n",
        "fig = plt.figure(dpi=200)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "im=ax.scatter(x, y, z, c=predicted.flatten(), cmap='viridis', marker='o')\n",
        "ax.set_xlabel('Number of pixels [#]')\n",
        "fig.colorbar(im,ticks=[0,1,2],label='Label number', pad = 0.1, fraction = 0.05)\n",
        "plt.title('Prediction')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Prediction.png')\n"
      ],
      "metadata": {
        "id": "Sdv5EuAeizM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (5, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "targets=targets.cpu().detach().numpy() [0,:,:,:]\n",
        "x, y, z = np.meshgrid(np.arange(label1.shape[0]),np.arange(label1.shape[1]),np.arange(label1.shape[2]), indexing='ij')\n",
        "fig = plt.figure(dpi=200)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "im2=ax.scatter(x, y, z, c=targets.flatten(), cmap='viridis', marker='o')\n",
        "ax.set_xlabel('Number of pixels [#]')\n",
        "fig.colorbar(im2,ticks=[0,1,2],label='Label number', pad = 0.1, fraction = 0.05)\n",
        "plt.title('Target')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Target.png')\n",
        "\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (5, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "#Difference plot, example with one image\n",
        "fig = plt.figure(dpi=200)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "im3=ax.scatter(x, y, z, c=np.abs(targets.flatten()-predicted.flatten()), cmap='viridis', marker='o')\n",
        "ax.set_xlabel('Number of pixels [#]')\n",
        "fig.colorbar(im3,ticks=[0,1,2],label='Difference in label number', pad = 0.1, fraction = 0.05)\n",
        "plt.title('Absolute difference in labels')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Diff')\n",
        "\n",
        "\n",
        "#Splitting volume cube in slices\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(np.abs(targets[0,:,32,:]-predicted[:,32,:]))\n",
        "plt.colorbar()\n",
        "plt.title('Horizontal slice, layer 32')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Horizontal_slice_32')\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(np.abs(targets[0,:,:,15]-predicted[:,:,15]))\n",
        "plt.colorbar()\n",
        "plt.title('Vertical slice, layer 15')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Horizontal_slice_15')\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(np.abs(targets[0,:,45,:]-predicted[:,45,:]))\n",
        "plt.colorbar()\n",
        "plt.title('Horizontal slice, layer 45')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Horizontal_slice_45')\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(np.abs(targets[0,:,:,34]-predicted[:,:,34]))\n",
        "plt.colorbar()\n",
        "plt.title('Vertical slice, layer 34')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Horizontal_slice_34')"
      ],
      "metadata": {
        "id": "B60xZMRrgS0f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
